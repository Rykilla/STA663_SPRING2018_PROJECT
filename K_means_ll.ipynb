{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Means Algorithm and Initializations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section supplies the Python code for the K-means algorithm along with functions that implement the K-means++ and K-means|| initializations. We include helper functions for computing intermediary quantities of interest used in K-means algorithm and initialization methods. These include: an array of the squared distance to each centroid for all points in the dataset, the cost, and the centroid weights used in the K-means|| initialization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(xs, centroid, weights = np.array([1])):\n",
    "    \"\"\"Computes matrix of squared distance from each point to each centroid.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    xs : ndarray of n points in d dimensional Euclidean space (nxd)\n",
    "    centroid: ndarray of k centroids in d dimensional Euclidean space (kxd)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    distance: matrix of squared distances (nxk)\n",
    "    \"\"\"\n",
    "    if weights.all() == 1:\n",
    "        weights = np.tile(np.array([1]),xs.shape[0])\n",
    "\n",
    "    distance = weights[:,None]*np.sum((xs[:,None,:] - centroid)**2, axis=-1)\n",
    "    return distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(d):\n",
    "    \"\"\"Computes the cost of a set of points with respect to a collection of centroids\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : matrix of squared distances (nxk); likely returned from distance() function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    cost: cost with respect to centroids\n",
    "    \"\"\"\n",
    "    #calculate distance to the nearest centroid for each point\n",
    "    min_dist = np.min(d, axis = 1)\n",
    "    \n",
    "    #compute cost\n",
    "    cost = np.sum(min_dist)\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def centroid_weights(d):\n",
    "    \"\"\"Computes weights as defined in step 7 of the k-means|| algorithm\n",
    "        \n",
    "    Parameters:\n",
    "    -----------\n",
    "    d : matrix of squared distances (nxk); likely returned from distance() function\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    w_x: ndarray of weights applied to centroids (kx1)\n",
    "    \"\"\"\n",
    "    #identify closest centroid to each point\n",
    "    c_close = np.zeros(d.shape)\n",
    "    c_close[np.arange(d.shape[0]), np.argmin(d, axis = 1)] = 1\n",
    "    \n",
    "    #compute the weights\n",
    "    w_x = np.sum(c_close, axis = 0)\n",
    "    return w_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means ++ initialization (Section 3.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_pp(xs, k, seed=None, verbose=False, weights = np.array([1])):\n",
    "    \"\"\"\n",
    "    Implements the K_means++ Initialization algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    xs: input dataset\n",
    "    k: the number of output clusters\n",
    "    seed: an optional random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    C: the reclustered k centroids used to initialize the k-means algorithm\n",
    "    \"\"\"\n",
    "    #initialization\n",
    "    np.random.seed(seed)\n",
    "    C = xs[np.random.choice(xs.shape[0],1),:]\n",
    "    loop = 0\n",
    "    \n",
    "    while len(C)<k:\n",
    "        \n",
    "        if ((loop % 10 == 0)&(verbose == True)):\n",
    "            print(\"The current loop is:\", loop)\n",
    "        \n",
    "        dist = distance(xs,C, weights = weights)\n",
    "        cst = cost(dist)\n",
    "        \n",
    "        probs_x = np.min(dist, axis = 1)/cst\n",
    "        C_new = xs[np.random.choice(xs.shape[0],1,p=probs_x),:]\n",
    "        \n",
    "        C = np.vstack((C,C_new))\n",
    "        \n",
    "        loop += 1\n",
    "        \n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means || initialization (Section 3.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Means_parallel(xs, k, l, seed=None, max_iter=None):\n",
    "    \"\"\"Implements the K_means || algorithm\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    xs : ndarray of n points in d dimensional Euclidean space (nxd)\n",
    "    k: the number of output clusters\n",
    "    l: the oversampling factor; the number of centroids to sample at each iteration\n",
    "    seed: an optional random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    C: the reclustered k centroids used to initialize the k-means algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialization\n",
    "    np.random.seed(seed)\n",
    "    centroid = xs[np.random.choice(xs.shape[0],1),:]\n",
    "    cost_int = cost(distance(xs,centroid))\n",
    "    \n",
    "    if cost_int == 0:\n",
    "        order = 0\n",
    "    else:\n",
    "        order = np.log10(cost_int)\n",
    "    \n",
    "    if max_iter is not None:\n",
    "        n_iter = max_iter\n",
    "    else:\n",
    "        n_iter = np.round(order)\n",
    "    \n",
    "    for i in np.arange(n_iter):\n",
    "        dist = distance(xs,centroid)\n",
    "        cst = cost(dist)\n",
    "        \n",
    "        probs_x = l*np.min(dist, axis = 1)/cst\n",
    "        centroid_new = xs[np.random.binomial(1, p = probs_x) == 1,:]\n",
    "        #centroid_new = xs[np.random.choice(xs.shape[0],l,p=probs_x),:] //old method\n",
    "        \n",
    "        centroid = np.vstack((centroid,centroid_new))\n",
    "        \n",
    "    dist = distance(xs,centroid)\n",
    "    w_x = centroid_weights(dist)\n",
    "    \n",
    "    #Implement k-means++ to recluster weighted points in C\n",
    "    #w_C = weight[:,None]*centroid\n",
    "    C = k_means_pp(centroid,k,seed=seed,weights=w_x)\n",
    "\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(123)\n",
    "l=2\n",
    "xs = GM[0]\n",
    "cent = xs[np.random.choice(xs.shape[0],5),:]\n",
    "\n",
    "probs_x = l*np.min(distance(xs,cent), axis = 1)/cost(distance(xs,cent))\n",
    "xs[np.random.binomial(1, p = probs_x) == 1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K_Means_parallel(xs, 5, l=2, seed=123, n_iter=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means(X, k, centroids, verbose=False):\n",
    "    \"\"\"\n",
    "        This function will separate X into k clusters using the classic k-means\n",
    "        algorithm.\n",
    "    \"\"\"\n",
    "    ## parameters\n",
    "    max_iter = 10000\n",
    "    step = 0\n",
    "    #n, p = X.shape\n",
    "    \n",
    "    ## run the algorithm\n",
    "    while step < max_iter:\n",
    "        ### sort the data in terms of clusters\n",
    "        dist = distance(X, centroids)\n",
    "        cluster_indices = np.argmin(dist, axis=1)\n",
    "        \n",
    "        ### update centroids\n",
    "        update_centroids = np.zeros(centroids.shape)\n",
    "        for i in range(k):\n",
    "            update_centroids[i,:] = np.mean(X[cluster_indices==i,:], axis=0)\n",
    "        \n",
    "        ### check conditions\n",
    "        if np.array_equal(update_centroids, centroids):\n",
    "            break\n",
    "        else:\n",
    "            centroids = update_centroids\n",
    "            \n",
    "            if ((step % 5 == 0)&(verbose == True)):\n",
    "                print(\"We are currently at {} step\".format(step))\n",
    "            \n",
    "            step += 1\n",
    "    \n",
    "    total_dist = distance(X, centroids)\n",
    "    total_cost = cost(total_dist)\n",
    "            \n",
    "    return {\"Centroids\": centroids,\n",
    "            \"Cluster Indices\": cluster_indices,\n",
    "            \"Number of Iterations\": step,\n",
    "            \"Total Cost\": total_cost}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization using C++"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Parallel Implementation (Section 3.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While both the K-means++ and K-means|| initialization methods select new centers according to a non-uniform distribution, the latter "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist(point, centroid):\n",
    "    \"\"\"Computes the squared distance to the nearest centroid for a given data point\"\"\"\n",
    "    min_dist = np.min(np.sum((point - centroid)**2, axis=-1))\n",
    "    return min_dist\n",
    "\n",
    "def close_cent(point, centroid):\n",
    "    \"\"\"Returns the index of the closest centroid to a given data point\"\"\"\n",
    "    index = np.argmin(np.sum((point - centroid)**2, axis=-1))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Functions in Parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_x_p(xs, centroid, cpu=None):\n",
    "    \"\"\"Computes the probabilities for sampling a new centroid(s)\n",
    "    for a given set of centroids and the data in parallel\n",
    "    \n",
    "    Also returns the intermediary cost value\n",
    "    \"\"\"\n",
    "    p_min_dist = partial(min_dist, centroid=centroid)\n",
    "    with Pool(processes = cpu) as pool:\n",
    "        min_d = pool.map(p_min_dist,xs)\n",
    "        cost = np.sum(min_d)\n",
    "        prob_x = min_d/cost\n",
    "    return cost, prob_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_p(xs, centroid, cpu=None):\n",
    "    \"\"\"Computes the weights for each centroid in parallel\"\"\"\n",
    "    \n",
    "    p_close_cent = partial(close_cent, centroid=centroid)\n",
    "    with Pool(processes = cpu) as pool:\n",
    "        indeces = pool.map(p_close_cent, xs)\n",
    "        w_x = np.array([Counter(indeces)[i] for i in np.arange(centroid.shape[0])])\n",
    "    return w_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Means_parallel_p(xs, k, l, seed=None, max_iter=None):\n",
    "    \"\"\"Implements the K_means || algorithm using parallel intermediary functions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    xs : ndarray of n points in d dimensional Euclidean space (nxd)\n",
    "    k: the number of output clusters\n",
    "    l: the oversampling factor; the number of centroids to sample at each iteration\n",
    "    seed: an optional random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    C: the reclustered k centroids used to initialize the k-means algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialization\n",
    "    np.random.seed(seed)\n",
    "    centroid = xs[np.random.choice(xs.shape[0],1),:]\n",
    "    cost_int = probs_x_p(xs,centroid)[0]\n",
    "    \n",
    "    if cost_int == 0:\n",
    "        order = 0\n",
    "    else:\n",
    "        order = np.log10(cost_int)\n",
    "    \n",
    "    if max_iter is not None:\n",
    "        n_iter = max_iter\n",
    "    else:\n",
    "        n_iter = np.round(order)\n",
    "    \n",
    "    n_iter = min(np.round(order),n_iter)\n",
    "    \n",
    "    for i in np.arange(n_iter):\n",
    "        \n",
    "        probs_x = l*probs_x_p(xs,centroid)[1]\n",
    "        centroid_new = xs[np.random.binomial(1, p = probs_x) == 1,:]\n",
    "        #centroid_new = xs[np.random.choice(xs.shape[0],l,p=probs_x),:]\n",
    "        \n",
    "        centroid = np.vstack((centroid,centroid_new))\n",
    "        \n",
    "    weight = weights_p(xs,centroid)\n",
    "    \n",
    "    #Implement k-means++ to recluster weighted points in C\n",
    "    w_C = weight[:,None]*centroid\n",
    "    C = k_means_pp(w_C,k,seed=seed)\n",
    "    #C = centroid[np.random.choice(centroid.shape[0],k,replace=False,p=weight),:]\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = GM[0] #data is from GaussMix dataset\n",
    "cent = xs[np.random.choice(xs.shape[0],5),:]\n",
    "\n",
    "probs_x_p(xs,cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_p(xs,cent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using iPyParallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipyparallel import Client\n",
    "rc = Client()\n",
    "dv = rc[:]\n",
    "with dv.sync_imports():\n",
    "    import numpy\n",
    "\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_dist_ipy(point, centroid):\n",
    "    \"\"\"Computes the squared distance to the nearest centroid for a given data point\"\"\"\n",
    "    min_dist = numpy.min(numpy.sum((point - centroid)**2, axis=-1))\n",
    "    return min_dist\n",
    "\n",
    "def close_cent_ipy(point, centroid):\n",
    "    \"\"\"Returns the index of the closest centroid to a given data point\"\"\"\n",
    "    index = numpy.argmin(numpy.sum((point - centroid)**2, axis=-1))\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def probs_x_ipy(xs, l, centroid, cpu=None):\n",
    "    \"\"\"Computes the probabilities for sampling a new centroid(s)\n",
    "    for a given set of centroids and the data in parallel\n",
    "    \n",
    "    Also returns the intermediary cost value\n",
    "    \"\"\"\n",
    "    p_min_dist = partial(min_dist_ipy, centroid=centroid)\n",
    "    min_d = dv.map_sync(p_min_dist,xs)\n",
    "    \n",
    "    cost = numpy.sum(min_d)\n",
    "    prob_x = l*min_d/cost\n",
    "    return cost, prob_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_ipy(xs, centroid, cpu=None):\n",
    "    \"\"\"Computes the weights for each centroid in parallel\"\"\"\n",
    "    \n",
    "    p_close_cent = partial(close_cent_ipy, centroid=centroid)\n",
    "    indeces = dv.map_sync(p_close_cent, xs)\n",
    "    \n",
    "    #w_x = np.array([Counter(indeces)[i] for i in np.arange(centroid.shape[0])])\n",
    "    return indeces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p_min_dist_ipy = partial(min_dist_ipy, centroid=cent)\n",
    "probs_x_ipy(xs,cent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Means_parallel_ipy(xs, k, l, seed=None, max_iter=None):\n",
    "    \"\"\"Implements the K_means || algorithm using parallel intermediary functions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    xs : ndarray of n points in d dimensional Euclidean space (nxd)\n",
    "    k: the number of output clusters\n",
    "    l: the oversampling factor; the number of centroids to sample at each iteration\n",
    "    seed: an optional random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    C: the reclustered k centroids used to initialize the k-means algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialization\n",
    "    np.random.seed(seed)\n",
    "    centroid = xs[np.random.choice(xs.shape[0],1),:]\n",
    "    cost_int = probs_x_ipy(xs,centroid)[0]\n",
    "    \n",
    "    if cost_int == 0:\n",
    "        order = 0\n",
    "    else:\n",
    "        order = np.log10(cost_int)\n",
    "    \n",
    "    if max_iter is not None:\n",
    "        n_iter = max_iter\n",
    "    else:\n",
    "        n_iter = np.round(order)\n",
    "    \n",
    "    for i in np.arange(n_iter):\n",
    "        \n",
    "        probs_x = l*probs_x_ipy(xs,centroid)[1]\n",
    "        centroid_new = xs[np.random.binomial(1, p = probs_x) == 1,:]\n",
    "        #centroid_new = xs[np.random.choice(xs.shape[0],l,p=probs_x),:]\n",
    "        \n",
    "        centroid = np.vstack((centroid,centroid_new))\n",
    "        \n",
    "    idx = weights_ipy(xs,centroid)\n",
    "    weight = np.array([Counter(idx)[i] for i in np.arange(centroid.shape[0])])\n",
    "    \n",
    "    #Implement k-means++ to recluster weighted points in C\n",
    "    w_C = weight[:,None]*centroid\n",
    "    C = k_means_pp(w_C,k,seed=seed)\n",
    "    \n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "K_Means_parallel_ipy(xs1,5,5,seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the Parallel Decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dv.parallel(block = True)\n",
    "def f4(x, y):\n",
    "    return x+y\n",
    "\n",
    "\n",
    "#f4(np.arange(10),np.arange(10))\n",
    "cent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = GM[0]\n",
    "cent = xs[np.random.choice(xs.shape[0],5),:]\n",
    "mydict=dict(centroid=cent, l=4, sd=12345, cost = np.sum(cost_ll(distance_ll(xs))))\n",
    "dv.push(mydict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dv.parallel(block=True)\n",
    "def distance_ll(x):\n",
    "    dist = numpy.sum((x[:,None,:] - centroid)**2, axis=-1)\n",
    "    return dist\n",
    "\n",
    "\n",
    "@dv.parallel(block=True)\n",
    "def cost_ll(dist):\n",
    "    min_dist = numpy.min(dist, axis=1)\n",
    "    cost = numpy.sum(min_dist)\n",
    "    return cost\n",
    "\n",
    "@dv.parallel(block=True)\n",
    "def sample_cent(dist):\n",
    "    numpy.random.seed(sd)\n",
    "    min_dist = numpy.min(dist, axis=1)\n",
    "    prob_x = l*min_dist/cost\n",
    "    idx = numpy.random.binomial(1, p = prob_x)\n",
    "    return idx\n",
    "\n",
    "@dv.parallel(block=True)\n",
    "def weights_ll(dist):\n",
    "    c_close = numpy.zeros(dist.shape)\n",
    "    c_close[numpy.arange(dist.shape[0]), numpy.argmin(dist, axis = 1)] = 1\n",
    "    \n",
    "    return c_close\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost(distance(xs,cent)), np.sum(cost_ll(distance_ll(xs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(12345)\n",
    "probs_x = 4*np.min(distance(xs,cent), axis = 1)/cost(distance(xs,cent))\n",
    "\n",
    "np.nonzero(sample_cent(distance_ll(xs))), np.nonzero(np.random.binomial(1,p=probs_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def K_Means_parallel_ll(xs, k, l, seed=None, max_iter=None):\n",
    "    \"\"\"Implements the K_means || algorithm using parallel intermediary functions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    xs : ndarray of n points in d dimensional Euclidean space (nxd)\n",
    "    k: the number of output clusters\n",
    "    l: the oversampling factor; the number of centroids to sample at each iteration\n",
    "    seed: an optional random seed\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    C: the reclustered k centroids used to initialize the k-means algorithm\n",
    "    \"\"\"\n",
    "    \n",
    "    #initialization\n",
    "    np.random.seed(seed)\n",
    "    centroid = xs[np.random.choice(xs.shape[0],1),:]\n",
    "    dv.push(dict(centroid=centroid, l=l, sd=seed))\n",
    "    cost_int = np.sum(cost_ll(distance_ll(xs)))\n",
    "    \n",
    "    if cost_int == 0:\n",
    "        order = 0\n",
    "    else:\n",
    "        order = np.log10(cost_int)\n",
    "    \n",
    "    if max_iter is not None:\n",
    "        n_iter = max_iter\n",
    "    else:\n",
    "        n_iter = np.round(order)\n",
    "    \n",
    "    for i in np.arange(n_iter):\n",
    "        \n",
    "        \n",
    "        dist = distance_ll(xs)\n",
    "        cost = np.sum(cost_ll(dist)) #new method\n",
    "        dv.push(dict(cost=cost)) #new method\n",
    "        centroid_new = xs[sample_cent(dist) == 1,:] #new method\n",
    "        \n",
    "        #probs_x = l*np.min(dist, axis = 1)/cost\n",
    "        #centroid_new = xs[np.random.binomial(1,p=probs_x)==1,:]\n",
    "        \n",
    "        #probs_x = np.min(dist, axis = 1)/np.sum(cost_ll(dist))\n",
    "        #centroid_new = xs[np.random.choice(xs.shape[0],l,p=probs_x),:]\n",
    "        \n",
    "        centroid = np.vstack((centroid,centroid_new))\n",
    "        dv.push(dict(centroid=centroid))\n",
    "    \n",
    "    weight = np.sum(weights_ll(distance_ll(xs)),axis=0)\n",
    "    \n",
    "    #Implement k-means++ to recluster weighted points in C\n",
    "    w_C = weight[:,None]*centroid\n",
    "    C = k_means_pp(w_C,k,seed=seed)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = GM[0]\n",
    "xs1 = GaussMix(10,k,n=400000,seed=12345)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "K_Means_parallel(xs1, k=5, l=5, seed=12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "K_Means_parallel_ll(xs1, k=5, l=5, seed=12345)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate GaussMixture synthetic data for testing (Section 4.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We validate our algorithm on the GaussMixture synthetic dataset referenced in the paper. This dataset is a mixture of $k$ spherical Gaussians with equal weights generated by first sampling $k$ centers from a 15-dimensional spherical Gaussian distribution with $\\vec{0}$ and variance $R \\in \\{0,10,100\\}$. Points from univariate standard normal distributions are added around each center to generate the full synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GaussMix(R, k, n=10000, d=15, seed=None):\n",
    "    \"\"\"Generates GaussMixture synthetic dataset\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    mu = np.zeros(d)\n",
    "    sigma = np.diag(R*np.ones(d))\n",
    "    centers = np.random.multivariate_normal(mean=mu, cov=sigma, size=k)\n",
    "    \n",
    "    X,y = make_blobs(n_samples=n, n_features=d, centers=centers, random_state=seed)\n",
    "    return X,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k=10\n",
    "GM = GaussMix(10,k,seed=12345)\n",
    "df = pd.DataFrame(GM[0])\n",
    "y = GM[1]\n",
    "pd.plotting.scatter_matrix(df.iloc[:,0:5], c=y, figsize=(10,10),\n",
    "                           diagonal='kde', alpha=0.5, cmap='Spectral')\n",
    "pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validating our Algorithm versus Existing Package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we proceed exploring the performance of the k-means algorithm using different initializations, we first compare the results of our algorithm from that of an existing function from scikit-learn. Using a random initializaiton, we compare the output of each method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = GM[0]\n",
    "np.random.seed(12345)\n",
    "c_int = xs[np.random.choice(xs.shape[0],k,replace=False),:]\n",
    "pred = k_means(xs,k,c_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "plib = KMeans(k,init='random',random_state=12345,n_init=1).fit(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.cm.Spectral(np.linspace(0,1,k))\n",
    "fig, ax = plt.subplots()\n",
    "for i in range(k):\n",
    "        points = np.array([xs[j,0:2] for j in range(xs.shape[0]) if plib.labels_[j] == i])\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=7, c=colors[i])\n",
    "ax.scatter(plib.cluster_centers_[:,0], plib.cluster_centers_[:,1], marker='*', s=200, c='black')\n",
    "plt.title('K-Means Clustering: sklearn', fontsize=15)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "for i in range(k):\n",
    "        points = np.array([xs[j,0:2] for j in range(xs.shape[0]) if pred['Cluster Indices'][j] == i])\n",
    "        ax.scatter(points[:, 0], points[:, 1], s=7, c=colors[i])\n",
    "ax.scatter(pred['Centroids'][:,0], pred['Centroids'][:,1], marker='*', s=200, c='black')\n",
    "plt.title('K-Means Clustering', fontsize=15)\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_error = np.sum(plib.labels_==pred['Cluster Indices'])/len(plib.labels_)\n",
    "\n",
    "print('Percent of points with same classification:', l_error)\n",
    "print('Final cost using sklearn:',cost(distance(xs,plib.cluster_centers_)))\n",
    "print('Final cost using our algorithm:',cost(distance(xs,pred['Centroids'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparison of initializations on Synthetic Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 50\n",
    "R = 1\n",
    "sd = 12345\n",
    "s_data, s_true_labels = GaussMix(R,k,seed=sd)\n",
    "\n",
    "c_init_r = s_data[np.random.choice(s_data.shape[0],k,replace=False),:]\n",
    "c_init_pp = k_means_pp(s_data, k, seed=sd)\n",
    "c_init_ll_1 = K_Means_parallel(s_data, k, l=k/2, seed=sd, max_iter=5)\n",
    "c_init_ll_2 = K_Means_parallel(s_data, k, l=2*k, seed=sd, max_iter=5)\n",
    "\n",
    "\n",
    "k_out_r = k_means(s_data,k,centroids=c_init_r)\n",
    "k_out_pp = k_means(s_data,k,centroids=c_init_pp)\n",
    "k_out_ll_1 = k_means(s_data,k,centroids=c_init_ll_1)\n",
    "k_out_ll_2 = k_means(s_data,k,centroids=c_init_ll_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final cost for random initialization: 144707.35282007643\n",
      "Final cost for k-means++ initialization: 145068.2892219253\n",
      "Final cost for k-means|| initialization with l=k/2 and r=5: 143827.5953339313\n",
      "Final cost for k-means|| initialization with l=2k and r=5: 144460.90310889774\n"
     ]
    }
   ],
   "source": [
    "print('Final cost for random initialization:', k_out_r['Total Cost'])\n",
    "print('Final cost for k-means++ initialization:', k_out_pp['Total Cost'])\n",
    "print('Final cost for k-means|| initialization with l=k/2 and r=5:', k_out_ll_1['Total Cost'])\n",
    "print('Final cost for k-means|| initialization with l=2k and r=5:', k_out_ll_2['Total Cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = distance(s_data,c_init_ll_1)\n",
    "cluster_indices = np.argmin(dist, axis=1)\n",
    "update_centroids = np.zeros(c_init_ll_1.shape)\n",
    "for i in range(k):\n",
    "    update_centroids[i,:] = np.mean(s_data[cluster_indices==i,:], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_init_ll_1 = K_Means_parallel(s_data, k, l=k/2, seed=sd, max_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = distance(s_data,c_init_ll_1)\n",
    "cluster_indices = np.argmin(dist, axis=1)\n",
    "set(cluster_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15,), (112,))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(1,5, size=15).shape, c_init_ll_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 15)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(123)\n",
    "c_init_ll_1[1][np.random.choice(c_init_ll_1[1].shape[0],1),:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112, 50)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distance(c_init_ll_1[1],c_init_r, weights = c_init_ll_1[0]).shape\n",
    "#distance(c_init_ll_1[1],c_init_r).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('kddcup.data_10_percent.gz', compression='gzip', header=None)\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((112, 15), (112,))"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_init_ll_1[1].shape, c_init_ll_1[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(112,)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.tile(np.array([1]),c_init_ll_1[1].shape[0]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
